{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ce870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('/home/kvulic/Vulic/cmos_toolbox_w_spike_sorter/')\n",
    "#from src.utils.logger_functions import console\n",
    "from src.cmos_plotter.Plotter_Helper_KV import *\n",
    "from src.cmos_plotter.Pair_activity_plotter import *\n",
    "\n",
    "from src.utils.metadata_functions import load_metadata_as_dataframe\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffa484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def split_pickle_by_time(input_file, input_dir=None, output_dir=None, segment_duration=30.0):\n",
    "    \"\"\"\n",
    "    Split a pickle file containing spike data into smaller pickle files based on time segments.\n",
    "    Adjusts spike timestamps to be relative to the start of each segment.\n",
    "    Ensures all unit indices from the original file are present in each segment.\n",
    "    \"\"\"\n",
    "    # Create output directory if not provided\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(os.path.abspath(input_file))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Handle input directory\n",
    "    if input_dir is None:\n",
    "        input_dir = os.path.dirname(os.path.abspath(input_file))\n",
    "    \n",
    "    # Get base filename without extension\n",
    "    base_filename = Path(input_file).stem\n",
    "    \n",
    "    # Load the pickle file\n",
    "    with open(os.path.join(input_dir, input_file), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Extract needed data\n",
    "    experiment_duration = data['EXPERIMENT_DURATION']\n",
    "    spikemat = data['SPIKEMAT']\n",
    "    spikemat_extremum = data['SPIKEMAT_EXTREMUM']\n",
    "    \n",
    "    # Get all unique unit indices from the original data\n",
    "    all_unit_indices = np.unique(spikemat_extremum['UnitIdx'])\n",
    "    \n",
    "    # Create a mapping of unit indices to their corresponding electrodes\n",
    "    unit_to_electrode = {}\n",
    "    for row in spikemat_extremum:\n",
    "        unit_to_electrode[row['UnitIdx']] = row['Electrode']\n",
    "    \n",
    "    # Calculate number of segments\n",
    "    num_segments = int(np.ceil(experiment_duration / segment_duration))\n",
    "    \n",
    "    # Process each segment\n",
    "    for segment_idx in range(num_segments):\n",
    "        # Calculate segment time boundaries in milliseconds\n",
    "        start_time_ms = segment_idx * segment_duration * 1000\n",
    "        end_time_ms = min((segment_idx + 1) * segment_duration * 1000, experiment_duration * 1000)\n",
    "        \n",
    "        # For the first segment, remove the first 1000ms\n",
    "        if segment_idx == 0:\n",
    "            start_time_ms += 1000\n",
    "        \n",
    "        # Filter SPIKEMAT data for this segment\n",
    "        segment_spikemat = spikemat[\n",
    "            (spikemat['Spike_Time'] >= start_time_ms) & \n",
    "            (spikemat['Spike_Time'] < end_time_ms)\n",
    "        ].copy()  # Make a copy to avoid modifying the original\n",
    "        \n",
    "        # Filter SPIKEMAT_EXTREMUM data for this segment\n",
    "        segment_spikemat_extremum = spikemat_extremum[\n",
    "            (spikemat_extremum['Spike_Time'] >= start_time_ms) & \n",
    "            (spikemat_extremum['Spike_Time'] < end_time_ms)\n",
    "        ].copy()  # Make a copy to avoid modifying the original\n",
    "        \n",
    "        # IMPORTANT: Adjust spike times to be relative to the start of this segment\n",
    "        segment_spikemat['Spike_Time'] -= start_time_ms\n",
    "        segment_spikemat_extremum['Spike_Time'] -= start_time_ms\n",
    "        \n",
    "        # Get unit indices that are present in this segment\n",
    "        present_unit_indices = np.unique(segment_spikemat_extremum['UnitIdx'])\n",
    "        \n",
    "        # Create dummy rows for missing unit indices\n",
    "        missing_units = np.setdiff1d(all_unit_indices, present_unit_indices)\n",
    "        \n",
    "        if len(missing_units) > 0:\n",
    "            # Create dummy rows for each missing unit\n",
    "            dummy_rows = []\n",
    "            \n",
    "            for unit_idx in missing_units:\n",
    "                # Get the electrode for this unit\n",
    "                electrode = unit_to_electrode.get(unit_idx, \"Unknown\")\n",
    "                \n",
    "                # Create a dummy row with the correct dtype\n",
    "                dummy_row = np.array([(electrode, 0.1, unit_idx)], \n",
    "                                     dtype=spikemat_extremum.dtype)\n",
    "                dummy_rows.append(dummy_row)\n",
    "            \n",
    "            # Combine existing data with dummy rows\n",
    "            if dummy_rows:\n",
    "                dummy_array = np.concatenate(dummy_rows)\n",
    "                segment_spikemat_extremum = np.concatenate([segment_spikemat_extremum, dummy_array])\n",
    "        \n",
    "        # Prepare segment data\n",
    "        segment_data = data.copy()  # Copy all fields from original data\n",
    "        segment_data['SPIKEMAT'] = segment_spikemat\n",
    "        segment_data['SPIKEMAT_EXTREMUM'] = segment_spikemat_extremum\n",
    "        segment_data['EXPERIMENT_DURATION'] = segment_duration  # Set to 30 seconds\n",
    "        \n",
    "        # For the first segment, adjust experiment duration if needed\n",
    "        if segment_idx == 0 and end_time_ms - start_time_ms < segment_duration * 1000:\n",
    "            segment_data['EXPERIMENT_DURATION'] = (end_time_ms - start_time_ms) / 1000\n",
    "        \n",
    "        # Save segment data to a new pickle file\n",
    "        output_file = os.path.join(output_dir, f\"{base_filename}_segment_{segment_idx+1}.pkl\")\n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(segment_data, f)\n",
    "        \n",
    "        # Count units in this segment\n",
    "        units_in_segment = len(np.unique(segment_spikemat_extremum['UnitIdx']))\n",
    "        \n",
    "        print(f\"Saved segment {segment_idx+1}/{num_segments} to {output_file}\")\n",
    "        print(f\"  Time range: {start_time_ms/1000:.3f}s - {end_time_ms/1000:.3f}s\")\n",
    "        print(f\"  Adjusted time range: 0.000s - {(end_time_ms-start_time_ms)/1000:.3f}s\")\n",
    "        print(f\"  SPIKEMAT entries: {len(segment_spikemat)}\")\n",
    "        print(f\"  SPIKEMAT_EXTREMUM entries: {len(segment_spikemat_extremum)}\")\n",
    "        print(f\"  Total units: {units_in_segment} (including {len(missing_units)} dummy units)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = '/itet-stor/kvulic/neuronies/single_neurons/1_Subprojects/Neurons_As_DNNs/3_Processed_Data/Nonos_data/Processed_2024_04_26_Spontaneous_alexschip/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_PATH = os.path.join(MAIN_PATH,'Sorters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(PROCESSED_DATA_PATH)\n",
    "filenames = [f for f in filenames if f.endswith('.pkl')]\n",
    "for i,filename in enumerate(filenames):\n",
    "    if i%10 == 0:\n",
    "            output_dir = os.path.join(PROCESSED_DATA_PATH, f'Split_files/Part_{i}')\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "    if \"1823\" in filename and \"N0\" not in filename and \"N4\" not in filename:\n",
    "        print(f'Processing {filename}')\n",
    "        #Split the file into segments \n",
    "        \n",
    "        split_pickle_by_time(filename, input_dir = PROCESSED_DATA_PATH, output_dir=output_dir, segment_duration=120.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
