{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aad9ae06686530",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:58:46.482276Z",
     "start_time": "2025-05-08T17:58:45.086922Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202078cdfdc3a9c1",
   "metadata": {},
   "source": [
    "# Merge the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe112404ff7a8b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T17:58:46.515111Z",
     "start_time": "2025-05-08T17:58:46.493050Z"
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing the pickle files\n",
    "DATA_DIR = '/itet-stor/kvulic/neuronies/single_neurons/1_Subprojects/Neurons_As_DNNs/3_Processed_Data/March2025_heart/biTE_stimulation/Full_dataset/Full_files'\n",
    "OUTPUT_DIR = '/itet-stor/kvulic/neuronies/single_neurons/1_Subprojects/Neurons_As_DNNs/3_Processed_Data/March2025_heart/biTE_stimulation/Full_dataset/CSVs'\n",
    "\n",
    "DATA_KEYS = [\"validated_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75306a2c83892",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-08T17:58:47.432490Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# List to store all dataframes\n",
    "all_data = []\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Dictionary to store all extracted data\n",
    "data_dict = {key: [] for key in DATA_KEYS}\n",
    "\n",
    "# Loop through all pickle files in the directory\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        file_path = os.path.join(DATA_DIR, file)\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "\n",
    "            for key in DATA_KEYS:\n",
    "                if key in data:\n",
    "                    df_data = pd.DataFrame(data[key]) # Convert structured array to DataFrame\n",
    "                    df = df_data[[\"source_electrode\", \"target_electrode\",\n",
    "                      \"source_unit_id\", \"target_unit_id\",\n",
    "                      \"lag\", \"validation\", \"mTE\",\n",
    "                      \"syn probability\", \"latency_extremum\"]\n",
    "                    ]\n",
    "\n",
    "                    electrodes_sources = []\n",
    "                    electrodes_targets = []\n",
    "                    for _, row in df.iterrows():\n",
    "                        electrodes_sources.append(data[\"UNIT_TO_EL\"][row[\"source_unit_id\"]])\n",
    "                        electrodes_targets.append(data[\"UNIT_TO_EL\"][row[\"target_unit_id\"]])\n",
    "\n",
    "                    df[\"electrodes_source_unit\"] = electrodes_sources\n",
    "                    df[\"electrodes_target_unit\"] = electrodes_targets\n",
    "                    df[\"filename\"] = file\n",
    "                    data_dict[key].append(df)\n",
    "                \n",
    "                else:\n",
    "                    print(f\"Warning: key {key} was not in file {file}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "\n",
    "# Save each dataset as a separate CSV\n",
    "for key in DATA_KEYS:\n",
    "    if data_dict[key]:\n",
    "        merged_df = pd.concat(data_dict[key], ignore_index=True)\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{key}_full_data_w_speed_and_firing.pkl\")\n",
    "        #merged_df.to_csv(output_file, index=False)\n",
    "        with open(output_file, \"wb\") as f: \n",
    "            pickle.dump(merged_df, f)\n",
    "        print(f\"Saved {key} data to {output_file}\")\n",
    "    else:\n",
    "        print(f\"No valid {key} data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aaec6948f63a1d",
   "metadata": {},
   "source": [
    "# Add information to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b06e56d2f6383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:38:42.280512Z",
     "start_time": "2025-05-06T09:38:41.807238Z"
    }
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv(r\"Z:\\neuronies\\single_neurons\\1_Subprojects\\Neurons_As_DNNs\\3_Processed_Data\\March2025_heart\\biTE_stimulation\\Full_dataset\\CSVs\\validated_results_full_data.csv\")\n",
    "data = np.load('/itet-stor/kvulic/neuronies/single_neurons/1_Subprojects/Neurons_As_DNNs/3_Processed_Data/March2025_heart/biTE_stimulation/Full_dataset/CSVs/validated_results_full_data_w_speed_and_firing.pkl', allow_pickle=True)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf138da9418835a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:38:44.794726Z",
     "start_time": "2025-05-06T09:38:44.779606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to parse the paste.txt content and create a lookup dictionary\n",
    "def parse_experiment_info(text_content):\n",
    "    # Split the content by sections (each experiment)\n",
    "    sections = re.split(r'-{20,}', text_content)\n",
    "\n",
    "    # Dictionary to store experiment information by filename\n",
    "    experiment_lookup = {}\n",
    "\n",
    "    # Process each section\n",
    "    current_experiment = None\n",
    "    current_frequency = None\n",
    "    current_delay = None\n",
    "    current_chip_id = None\n",
    "    current_network = None\n",
    "    current_div = None\n",
    "    current_repetition = None\n",
    "    current_status = None\n",
    "\n",
    "    for section in sections:\n",
    "        if not section.strip():\n",
    "            continue\n",
    "\n",
    "        # Extract experiment details\n",
    "        exp_match = re.search(r'Experiment:\\s*([^\\n]+)', section)\n",
    "        if exp_match:\n",
    "            current_experiment = exp_match.group(1).strip()\n",
    "\n",
    "        freq_match = re.search(r'Stimulation frequency:\\s*([^\\n]+)', section)\n",
    "        if freq_match:\n",
    "            current_frequency = freq_match.group(1).strip()\n",
    "\n",
    "        delay_match = re.search(r'Delay:\\s*([^\\n]+)', section)\n",
    "        if delay_match:\n",
    "            current_delay = delay_match.group(1).strip()\n",
    "\n",
    "        chip_match = re.search(r'Chip ID:\\s*([^\\n]+)', section)\n",
    "        if chip_match:\n",
    "            current_chip_id = chip_match.group(1).strip()\n",
    "\n",
    "        network_match = re.search(r'Network:\\s*([^\\n]+)', section)\n",
    "        if network_match:\n",
    "            current_network = network_match.group(1).strip()\n",
    "\n",
    "        div_match = re.search(r'DIV:\\s*([^\\n]+)', section)\n",
    "        if div_match:\n",
    "            current_div = div_match.group(1).strip()\n",
    "\n",
    "        rep_match = re.search(r'Repetition:\\s*([^\\n]+)', section)\n",
    "        if rep_match:\n",
    "            current_repetition = rep_match.group(1).strip()\n",
    "\n",
    "        # Process status and filename pairs\n",
    "        status_file_pairs = re.findall(r'Status:\\s*(before|after|after_2)\\s*\\n(ID\\d+_\\d+_DIV\\d+_DATE\\d+_\\d+_[^\\.]+\\.raw_processed_info_metrics\\.pkl)', section, re.IGNORECASE)\n",
    "\n",
    "        for status, filename in status_file_pairs:\n",
    "            experiment_lookup[filename] = {\n",
    "                'Experiment': current_experiment,\n",
    "                'Stimulation_frequency': current_frequency,\n",
    "                'Delay': current_delay,\n",
    "                'Chip_ID': current_chip_id,\n",
    "                'Network': current_network,\n",
    "                'DIV': current_div,\n",
    "                'Repetition': current_repetition,\n",
    "                'Status': status.lower()\n",
    "            }\n",
    "\n",
    "    return experiment_lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae72355f876281f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:39:24.222488Z",
     "start_time": "2025-05-06T09:39:24.175297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read info text document\n",
    "#with open(r\"Z:\\neuronies\\single_neurons\\1_Subprojects\\Neurons_As_DNNs\\3_Processed_Data\\March2025_heart\\biTE_stimulation\\Full_dataset\\info_stimulation_experiments.txt\", 'r') as file:\n",
    "with open('/itet-stor/kvulic/neuronies/single_neurons/1_Subprojects/Neurons_As_DNNs/3_Processed_Data/March2025_heart/biTE_stimulation/Full_dataset/info_stimulation_experiments.txt', 'r') as file:\n",
    "    # Read the content of the file\n",
    "    text_content = file.read()\n",
    "\n",
    "# Create lookup dictionary from the paste.txt content\n",
    "experiment_lookup = parse_experiment_info(text_content)\n",
    "print(experiment_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf0ad1dee7a485",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:39:36.599179Z",
     "start_time": "2025-05-06T09:39:35.031815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to add experiment information to the dataframe\n",
    "def add_experiment_info(row, lookup_dict):\n",
    "    filename = row['filename']\n",
    "\n",
    "    # Get info from lookup dictionary\n",
    "    try:\n",
    "        if filename in lookup_dict:\n",
    "            info = lookup_dict[filename]\n",
    "            return pd.Series([\n",
    "                info.get('Experiment', None),\n",
    "                info.get('Stimulation_frequency', None),\n",
    "                info.get('Delay', None),\n",
    "                info.get('Chip_ID', None),\n",
    "                info.get('Network', None),\n",
    "                info.get('DIV', None),\n",
    "                info.get('Repetition', None),\n",
    "                info.get('Status', None)\n",
    "            ])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding filename {filename}\")\n",
    "\n",
    "# Add the new columns to your dataframe\n",
    "new_cols = ['Experiment', 'Stimulation_frequency', 'Delay', 'Chip_ID', 'Network', 'DIV', 'Repetition', \"Status\"]\n",
    "data[new_cols] = data.apply(lambda row: add_experiment_info(row, experiment_lookup), axis=1)\n",
    "\n",
    "# Display the updated dataframe\n",
    "print(data[['filename'] + new_cols].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef30f9224a2d98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:39:40.650796Z",
     "start_time": "2025-05-06T09:39:40.619447Z"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5543809a1438eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:40:53.899603Z",
     "start_time": "2025-05-06T09:40:48.780238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the updated dataframe to a new pickle or csv file\n",
    "#data.to_csv(r\"Z:\\neuronies\\single_neurons\\1_Subprojects\\Neurons_As_DNNs\\3_Processed_Data\\March2025_heart\\biTE_stimulation\\Full_dataset\\CSVs\\validated_results_full_data.csv\", index=False)\n",
    "data.to_pickle('/itet-stor/kvulic/neuronies/single_neurons/1_Subprojects/Neurons_As_DNNs/3_Processed_Data/March2025_heart/biTE_stimulation/Full_dataset/CSVs/validated_results_full_data_w_speed_and_firing.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
